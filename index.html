

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="A deep learning library targeting high-performance computing (HPC) applications with performance-critical inference and training needs.">
    <meta name="author" content="Berkeley Lab" >
    <link rel="icon" href="./favicon.png">

    <title> Inference-Engine </title>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link href="./css/pygments.css" rel="stylesheet">
    <link href="./css/font-awesome.min.css" rel="stylesheet">
    <link href="./css/local.css" rel="stylesheet">
      <link  href="./tipuesearch/tipuesearch.css" rel="stylesheet">

    <script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>
    <script src="./js/svg-pan-zoom.min.js"></script>
  </head>

  <body>

    <!-- Fixed navbar -->
    <div class="container-fluid mb-sm-4 mb-xl-2">
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
          <a class="navbar-brand" href="./index.html">Inference-Engine </a>
          <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar"
                  aria-expanded="false" aria-controls="navbar" aria-label="Toggle navigation">
                  <span class="navbar-toggler-icon">
          </button>

          <div id="navbar" class="navbar-collapse collapse">
            <ul class="navbar-nav">
                  <li class="nav-item">
                    <a class="nav-link" href="./lists/files.html">Source Files</a>
                  </li>
                <li class="nav-item">
                  <a class="nav-link" href="./lists/modules.html">Modules</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="./lists/procedures.html">Procedures</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="./lists/absint.html">Abstract Interfaces</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="./lists/types.html">Derived Types</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="./lists/programs.html">Programs</a>
                </li>
            </ul>
              <div class="d-flex align-items-end flex-grow-1">
                <form action="./search.html" role="search" class="ms-auto">
                  <input type="text" class="form-control" aria-label="Search" placeholder="Search" name="q" id="tipue_search_input" autocomplete="off" required>
                </form>
              </div>
          </div><!--/.nav-collapse -->
        </div>
      </nav>
    </div>

    <div class="container">
  <!-- Main component for a primary marketing message or call to action -->
    <div class="p-5 mb-4 bg-light border rounded-3" id="jumbotron">
      <p>A deep learning library targeting high-performance computing (HPC) applications with performance-critical inference and training needs.</p>
        <p> Find us on&hellip;</p>
      <p>
        <a class="btn btn-lg btn-primary" href="https://github.com/berkeleylab/inference-engine
https://github.com/berkeleylab/inference-engine" role="button">GitHub</a>
        
        
        
        
        <a class="btn btn-lg btn-danger" style="float:right" href="https://github.com/berkeleylab/inference-engine/releases" role="button">Download the Source</a>
      </p>
    </div>

      <div class="row" id='text'>
        <div class=col-md-8>
          <h1>Inference-Engine</h1>
          <div class="codehilite"><pre><span></span><code>  _        __                                                     <span class="ge">_            </span>
<span class="ge"> (_</span>)      / <span class="ge">_|                                                   (_</span>)           
  <span class="ge">_ _</span> __ | |_ ___ _ __ ___ _ __   ___ ___         ___ _ __   __ <span class="ge">_ _</span> _ __   ___ 
 | | &#39;_ \|  <span class="ge">_/ _</span> \ &#39;__/ <span class="ge">_ \ &#39;_</span> \ / __/ _ \  __   / <span class="ge">_ \ &#39;_</span> \ / <span class="ge">_` | | &#39;_</span> \ / _ \
 | | | | | ||  __/ | |  __/ | | | (_|  __/ |__| |  __/ | | | (_| | | | | |  __/
 |_|_| |_|_| \___|_|  \___|_| |_|\___\___|       \___|_| |_|\__, |_|_| |_|\___|
                                                             __/ |             
                                                            |___/              
</code></pre></div>

<h1 id="inference-engine">Inference-Engine</h1>
<p><a href="#overview">Overview</a> | <a href="#getting-started">Getting Started</a> | <a href="#documentation">Documentation</a></p>
<h2 id="overview">Overview</h2>
<p>Inference-Engine supports research in the training and deployment of neural-network surrogate models for computational science.
Inference-Engine also provides a platform for exploring and advancing the native parallel programming features of Fortran 2023 in the context of deep learning.
The language features of interest facilitate loop-level parallelism via the <code>do concurrent</code> construct and Single-Program, Multiple Data (SMPD) parallelism via "multi-image" (e.g., multithreaded or multiprocess) execution.
Toward these ends,</p>
<ul>
<li>Most Inference-Engine procedures are <code>pure</code> and thus satisfy a language requirement for invocation inside <code>do concurrent</code>,</li>
<li>The network training procedure uses <code>do concurrent</code> to expose automatic parallelization opportunities to compilers, and</li>
<li>Exploiting multi-image execution to speedup training is under investigation.</li>
</ul>
<p>To broaden support for the native parallel features, Inference-Engine's contributors also write compiler tests, bug reports, and patches; develop a parallel runtime library (<a href="https://go.lbl.gov/caffeine">Caffeine</a>); participate in the language standardization process; and provide example inference and training code for exercising and evaluating compilers' automatic parallelization capabilities on processors and accelerators, including Graphics Processing Units (GPUs).</p>
<p>Available optimizers:
* Stochastic gradient descent and
* Adam (recommended).</p>
<p>Supported network types:
* Feed-forward networks and
* Residual networks (for inference only).</p>
<p>Supported activation functions:
* Sigmoid,
* RELU,
* GELU,
* Swish, and
* Step (for inference only).</p>
<p>Please submit a pull request or an issue to add or request other optimizers, network types, or activation functions.</p>
<h2 id="getting-started">Getting Started</h2>
<h3 id="examples-and-demonstration-applications">Examples and demonstration applications</h3>
<p>The <a href="example">example</a> subdirectory contains demonstrations of several relatively simple use cases.
We recommend reviewing the examples to see how to handle basic tasks such as configuring a network training run or reading a neural network and using it to perform inference.</p>
<p>The <a href="demo">demo</a> subdirectory contains demonstration applications that depend on Inference-Engine but build separately due to requiring additional prerequisites such as NetCDF and HDF5.
The demonstration applications
 - Train a cloud microphysics model surrogate for the Intermediate Complexity Atmospheric Research (<a href="https://github.com/BerkeleyLab/icar/tree/neural-net">ICAR</a>) package,
 - Perform inference using a pretrained model for aerosol dynamics in the Energy Exascale Earth System (<a href="https://e3sm.org">E3SM</a>) package, and
 - Calculate ICAR cloud microphysics tensor component statistics that provide useful insights for training-data reduction.</p>
<h3 id="building-and-testing">Building and Testing</h3>
<p>Because this repository supports programming language research, the code exercises new language features in novel ways.
We recommend using any compiler's latest release or even building open-source compilers from source.
The <a href="https://github.com/rouson/handy-dandy/blob/main/src">handy-dandy</a> repository contains scripts capturing steps for building the <a href="https://github.com/llvm/llvm-project">LLVM</a> compiler suite.
The remainder of this section contains commands for building Inference-Engine with a recent Fortran compiler and the Fortran Package Manager ([<code>fpm</code>]) in your <code>PATH</code>.</p>
<h4 id="gnu-gfortran-13-or-higher-required">GNU (<code>gfortran</code>) 13 or higher required</h4>
<div class="codehilite"><pre><span></span><code>fpm test --compiler gfortran --profile release
</code></pre></div>

<h4 id="nag-nagfor">NAG (<code>nagfor</code>)</h4>
<div class="codehilite"><pre><span></span><code>fpm test --compiler nagfor --flag -fpp --profile release
</code></pre></div>

<h4 id="llvm-flang-new">LLVM (<code>flang-new</code>)</h4>
<p>Building with <code>flang-new</code> requires passing flags to enable the compiler's experimental support for assumed-rank entities:</p>
<div class="codehilite"><pre><span></span><code>fpm test --compiler flang-new --flag &quot;-mmlir -allow-assumed-rank -O3&quot;
</code></pre></div>

<h5 id="experimental-automatic-parallelization-of-do-concurrent-on-cpus"><em>Experimental:</em> Automatic parallelization of <code>do concurrent</code> on CPUs</h5>
<p>With the <code>amd_trunk_dev</code> branch of the <a href="https://github.com/ROCm/llvm-project">ROCm fork</a> fork of LLVM, automatic parallelization currently works for inference, e.g.</p>
<div class="codehilite"><pre><span></span><code>fpm run \
  --example concurrent-inferences \
  --compiler flang-new \
  --flag &quot;-mmlir -allow-assumed-rank -O3 -fopenmp -fdo-concurrent-parallel=host&quot; \
  -- --network model.json
</code></pre></div>

<p>where <code>model.json</code> must be a neural network in the <a href="https://www.json.org/json-en.html">JSON</a> format used by Inference-Engine and the companion <a href="https://go.lbl.gov/nexport">nexport</a> package.
Automatic parallelization for training is under development.</p>
<h4 id="intel-ifx">Intel (<code>ifx</code>)</h4>
<div class="codehilite"><pre><span></span><code>fpm test --compiler ifx --profile release --flag -O3
</code></pre></div>

<h5 id="experimental-automatic-offloading-of-do-concurrent-to-gpus"><em>Experimental:</em> Automatic offloading of <code>do concurrent</code> to GPUs</h5>
<p>This capability is under development with the goal to facilitate automatic GPU offloading via the following command:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">fpm</span><span class="w"> </span><span class="nv">test</span><span class="w"> </span><span class="o">--</span><span class="nv">compiler</span><span class="w"> </span><span class="nv">ifx</span><span class="w"> </span><span class="o">--</span><span class="nv">profile</span><span class="w"> </span><span class="nv">releae</span><span class="w"> </span><span class="o">--</span><span class="nv">flag</span><span class="w"> </span><span class="s2">&quot;-fopenmp-target-do-concurrent -qopenmp -fopenmp-targets=spir64 -O3&quot;</span>
</code></pre></div>

<h4 id="hpe-crayftnsh-under-development">HPE (<code>crayftn.sh</code>) -- under development</h4>
<p>Support for the Cray Compiler Environment (CCE) Fortran compiler is under development.
Building with the CCE <code>ftn</code> compiler wrapper requires an additional trivial wrapper
shell script. For example, create a file <code>crayftn.sh</code> with the following contents and
place this file's location in your <code>PATH</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="ch">#!/bin/bash</span>

ftn<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</code></pre></div>

<p>Then execute</p>
<div class="codehilite"><pre><span></span><code>fpm test --compiler crayftn.sh
</code></pre></div>

<h3 id="configuring-a-training-run">Configuring a training run</h3>
<p>Inference-Engine imports hyperparameters and network configurations to and from JSON files.
To see the expected file format, run the [print-training-configuration] example as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="c">% fpm run --example print-training-configuration --compiler gfortran</span>
</code></pre></div>

<p>which should produce output like the following:</p>
<div class="codehilite"><pre><span></span><code>Project is up to date
 {
     &quot;hyperparameters&quot;: {
         &quot;mini-batches&quot; : 10,
         &quot;learning rate&quot; : 1.50000000,
         &quot;optimizer&quot; : &quot;adam&quot;
     }
 ,
     &quot;network configuration&quot;: {
         &quot;skip connections&quot; : false,
         &quot;nodes per layer&quot; : [2,72,2],
         &quot;activation function&quot; : &quot;sigmoid&quot;
     }
 }
</code></pre></div>

<p>Inference-Engine's JSON file format is fragile: splitting or combining lines breaks the file reader.
Files with added or removed white space or reordered whole objects ("hyperparameters" or "network configuration") should work.
A future release will leverage the <a href="https://gitlab.com/everythingfunctional/rojff">rojff</a> JSON interface to allow for more flexible file formatting.</p>
<h3 id="training-a-neural-network">Training a neural network</h3>
<p>Running the following command will train a neural network to learn the saturated mixing ratio function that is one component of the ICAR SB04 cloud microphysics model (see the <a href="example/supporting-modules/saturated_mixing_ratio_m.f90">saturated_mixing_ratio_m</a> module for an implementation of the involved function):</p>
<div class="codehilite"><pre><span></span><code> fpm run --example learn-saturated-mixing-ratio --compiler gfortran --profile release -- --output-file sat-mix-rat.json
</code></pre></div>

<p>The following is representative output after 3000 epochs:</p>
<div class="codehilite"><pre><span></span><code> Initializing a new network
         Epoch | Cost Function| System_Clock | Nodes per Layer
         1000    0.79896E-04     4.8890      2,4,72,2,1
         2000    0.61259E-04     9.8345      2,4,72,2,1
         3000    0.45270E-04     14.864      2,4,72,2,1
</code></pre></div>

<p>The example program halts execution after reaching a cost-function threshold (which requires millions of epochws) or a maximum number of iterations or if the program detects a file named <code>stop</code> in the source-tree root directory.
Before halting, the program will print a table of expected and predicted saturated mixing ratio values across a range of input pressures and temperatures, wherein two the inputs have each been mapped to the unit interval [0,1].
The program also writes the neural network initial condition to <code>initial-network.json</code> and the final (trained) network to the file specified in the above command: <code>sat-mix-rat.json</code>.</p>
<h3 id="performing-inference">Performing inference</h3>
<p>Users with a PyTorch model may use <a href="https://go.lbl.gov/nexport">nexport</a> to export the model to JSON files that Inference-Engine can read.
Examples of performing inference using a neural-network JSON file are in <a href="example/concurrent-inferences.f90">example/concurrent-inferences</a>.</p>
<h2 id="documentation">Documentation</h2>
<p>Please see our <a href="https://berkeleylab.github.io/inference-engine/">GitHub Pages site</a> for HTML documentation generated by [<code>ford</code>] or generate documentaiton locally by installing <code>ford</code> and executing <code>ford ford.md</code>.</p>
        </div>
          <div class="col-md-4">
            <div class="card card-body bg-light">
              <h2 class="card-title">Developer Info</h2>
              <h4 class="card-text">Berkeley Lab</h4>
              <p class="card-text"></p>
                <div class="text-center"><div class="btn-group" role="group">
                    
                    <a class="btn btn-lg btn-primary" href="https://github.com/berkeleylab"><i class="fa fa-github fa-lg"></i></a>
                    
                    
                    
                    
                    
                    
                </div></div>
            </div>
          </div>
      </div>
        <div class="row">
          <hr>
          <div class="col-xs-6 col-sm-3">
            <div>
              <h3>Source Files</h3>
              <ul><li><a href='sourcefile/activation_strategy_m.f90.html'>activation_strategy_m.f90</a></li><li><a href='sourcefile/concurrent-inferences.f90.html'>concurrent-inferences.f90</a></li><li><a href='sourcefile/differentiable_activation_strategy_m.f90.html'>differentiable_activation_strategy_m.f90</a></li><li><a href='sourcefile/gelu_m.f90.html'>gelu_m.f90</a></li><li><a href='sourcefile/gelu_s.f90.html'>gelu_s.f90</a></li><li><a href='sourcefile/hyperparameters_m.f90.html'>hyperparameters_m.f90</a></li><li><a href='sourcefile/hyperparameters_s.f90.html'>hyperparameters_s.f90</a></li><li><a href='sourcefile/inference_engine_m.f90.html'>inference_engine_m.f90</a></li><li><a href='sourcefile/inference_engine_m_.f90.html'>inference_engine_m_.f90</a></li><li><a href='sourcefile/inference_engine_s.f90.html'>inference_engine_s.F90</a></li></ul>
            </div>
            <div>
              <ul>
                <li><a href="./lists/files.html"><em>All source files&hellip;</em></a></li>
              </ul>
            </div>
          </div>
          <div class="col-xs-6 col-sm-3">
            <div>
              <h3>Modules</h3>
              <ul><li><a href='module/activation_strategy_m.html'>activation_strategy_m</a></li><li><a href='module/addition_m.html'>addition_m</a></li><li><a href='module/differentiable_activation_strategy_m.html'>differentiable_activation_strategy_m</a></li><li><a href='module/exponentiation_m.html'>exponentiation_m</a></li><li><a href='module/gelu_m.html'>gelu_m</a></li><li><a href='module/hyperparameters_m.html'>hyperparameters_m</a></li><li><a href='module/inference_engine_m.html'>inference_engine_m</a></li><li><a href='module/inference_engine_m_.html'>inference_engine_m_</a></li><li><a href='module/input_output_pair_m.html'>input_output_pair_m</a></li><li><a href='module/kind_parameters_m.html'>kind_parameters_m</a></li></ul>
            </div>
            <div>
              <ul>
                <li><a href="./lists/modules.html"><em>All modules&hellip;</em></a></li>
              </ul>
            </div>
          </div>
          <div class="col-xs-6 col-sm-3">
            <div>
              <h3>Procedures</h3>
              <ul><li><a href='interface/activation.html'>activation</a></li><li><a href='interface/activation~2.html'>activation</a></li><li><a href='interface/activation~3.html'>activation</a></li><li><a href='interface/activation~4.html'>activation</a></li><li><a href='interface/activation~5.html'>activation</a></li><li><a href='interface/activation_derivative.html'>activation_derivative</a></li><li><a href='interface/activation_derivative~2.html'>activation_derivative</a></li><li><a href='interface/activation_derivative~3.html'>activation_derivative</a></li><li><a href='interface/activation_derivative~4.html'>activation_derivative</a></li><li><a href='proc/activation_factory_method.html'>activation_factory_method</a></li></ul>
            </div>
            <div>
              <ul>
                <li><a href="./lists/procedures.html"><em>All procedures&hellip;</em></a></li>
              </ul>
            </div>
          </div>
          <div class="col-xs-6 col-sm-3">
            <div>
              <h3>Derived Types</h3>
              <ul><li><a href='type/activation_strategy_t.html'>activation_strategy_t</a></li><li><a href='type/difference_t.html'>difference_t</a></li><li><a href='type/differentiable_activation_strategy_t.html'>differentiable_activation_strategy_t</a></li><li><a href='type/exchange_t.html'>exchange_t</a></li><li><a href='type/gelu_t.html'>gelu_t</a></li><li><a href='type/hyperparameters_t.html'>hyperparameters_t</a></li><li><a href='type/inference_engine_t.html'>inference_engine_t</a></li><li><a href='type/input_output_pair_t.html'>input_output_pair_t</a></li><li><a href='type/layer_t.html'>layer_t</a></li><li><a href='type/metadata_t.html'>metadata_t</a></li></ul>
            </div>
            <div>
              <ul>
                <li><a href="./lists/types.html"><em>All derived types&hellip;</em></a></li>
              </ul>
            </div>
          </div>
        </div>
      <hr>
    </div> <!-- /container -->
    <footer>
      <div class="container">
        <div class="row justify-content-between">
          <div class="col">
            <p>
              Inference-Engine
 was developed by Berkeley Lab<br>              &copy; 2024 
</p>
          </div>
          <div class="col">
            <p class="text-end">
              Documentation generated by
              <a href="https://github.com/Fortran-FOSS-Programmers/ford">FORD</a>
 on 2024-10-01 22:01              </p>
          </div>
        </div>
        <br>
      </div> <!-- /container -->
    </footer>

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>    

    <!-- MathJax JavaScript
             ================================================== -->
             <!-- Placed at the end of the document so the pages load faster -->
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
          TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },
          jax: ['input/TeX','input/MathML','output/HTML-CSS'],
          extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']
          });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

          <script src="./tipuesearch/tipuesearch_content.js"></script>
          <script src="./tipuesearch/tipuesearch_set.js"></script>
          <script src="./tipuesearch/tipuesearch.js"></script>

  </body>
</html>