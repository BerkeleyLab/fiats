

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="Functional inference and training of surrogate models for computational science.">
    <meta name="author" content="Berkeley Lab" >
    <link rel="icon" href="./favicon.png">

    <title> Fiats </title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <!-- Font Awesome -->
    <link href="./css/fontawesome.min.css" rel="stylesheet">
    <link href="./css/brands.min.css" rel="stylesheet">
    <link href="./css/regular.min.css" rel="stylesheet">
    <link href="./css/solid.min.css" rel="stylesheet">
    <link href="./css/v4-font-face.min.css" rel="stylesheet">
    <link href="./css/v4-shims.min.css" rel="stylesheet">
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
      });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async
            integrity="sha256-DViIOMYdwlM/axqoGDPeUyf0urLoHMN4QACBKyB58Uw=" crossorigin="anonymous"></script>
    <!-- Other scripts and stylesheets -->
    <link href="./css/local.css" rel="stylesheet">
    <link href="./css/pygments.css" rel="stylesheet">
    <script src="./js/svg-pan-zoom.min.js"></script>
  </head>

  <body>

    <!-- Fixed navbar -->
    <div class="container-fluid mb-sm-4 mb-xl-2">
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
          <a class="navbar-brand" href="./index.html">Fiats </a>
          <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar"
                  aria-expanded="false" aria-controls="navbar" aria-label="Toggle navigation">
                  <span class="navbar-toggler-icon">
          </button>

          <div id="navbar" class="navbar-collapse collapse">
            <ul class="navbar-nav">
                  <li class="nav-item">
                    <a class="nav-link" href="./lists/files.html">Source Files</a>
                  </li>
                <li class="nav-item">
                  <a class="nav-link" href="./lists/modules.html">Modules</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="./lists/procedures.html">Procedures</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="./lists/types.html">Derived Types</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="./lists/programs.html">Programs</a>
                </li>
            </ul>
              <div class="d-flex align-items-end flex-grow-1">
                <form action="./search.html" role="search" class="ms-auto">
                  <input type="text" class="form-control" aria-label="Search" placeholder="Search" name="q" id="tipue_search_input" autocomplete="off" required>
                </form>
              </div>
          </div><!--/.nav-collapse -->
        </div>
      </nav>
    </div>

    <div class="container">
  <!-- Main component for a primary marketing message or call to action -->
    <div class="p-5 mb-4 bg-light border rounded-3" id="jumbotron">
      <p>Functional inference and training of surrogate models for computational science.</p>
        <p> Find us on&hellip;</p>
      <p>
        <a class="btn btn-lg btn-primary" href="https://github.com/berkeleylab/fiats
https://github.com/berkeleylab/fiats" role="button">GitHub</a>
        
        
        
        
        <a class="btn btn-lg btn-danger" style="float:right" href="https://github.com/berkeleylab/fiats/releases" role="button">Download the Source</a>
      </p>
    </div>

      <div class="row" id='text'>
        <div class=col-md-8>
          <h1>Fiats</h1>
          <div class="codehilite"><pre><span></span><code>___________.__        __          
\_   _____/|__|____ <span class="ge">_/  |_</span>  ______
 |    __)  |  \__  \\   __\/  ___/
 |     \   |  |/ __ \|  |  \___ \ 
 \___  /   |__(____  /__| /____  &gt;
     \/            \/          \/ 
</code></pre></div>

<h1 id="fiats-functional-inference-and-training-for-surrogates">Fiats: Functional inference and training for surrogates</h1>
<p>Alternatively, <em>Fortran inference and training for science</em>.</p>
<p><a href="#overview">Overview</a> | <a href="#getting-started">Getting Started</a> | <a href="#documentation">Documentation</a></p>
<h2 id="overview">Overview</h2>
<p>Fiats supports research on the training and deployment of neural-network surrogate models for computational science.
Fiats also provides a platform for exploring and advancing the native parallel programming features of Fortran 2023 in the context of deep learning.
The design of Fiats centers around functional programming patterns that facilitate concurrency, including loop-level parallelism via the <code>do concurrent</code> construct and Single-Program, Multiple Data (SMPD) parallelism via "multi-image" (e.g., multithreaded or multiprocess) execution.
Towards these ends,</p>
<ul>
<li>Most Fiats procedures are <code>pure</code> and thus satisfy a language requirement for invocation inside <code>do concurrent</code>,</li>
<li>The network training procedure uses <code>do concurrent</code> to expose automatic parallelization opportunities to compilers, and</li>
<li>Exploiting multi-image execution to speedup training is under investigation.</li>
</ul>
<p>To broaden support for the native parallel features, the Fiats contributors also write compiler tests, bug reports, and patches; develop a parallel runtime library (<a href="https://go.lbl.gov/caffeine">Caffeine</a>); participate in the language standardization process; and provide example inference and training code for exercising and evaluating compilers' automatic parallelization capabilities on processors and accelerators, including Graphics Processing Units (GPUs).</p>
<p>Available optimizers:</p>
<ul>
<li>Stochastic gradient descent and</li>
<li>Adam (recommended).</li>
</ul>
<p>Supported network types:</p>
<ul>
<li>Feed-forward networks and</li>
<li>Residual networks (for inference only).</li>
</ul>
<p>Supported activation functions:</p>
<ul>
<li>Sigmoid,</li>
<li>RELU,</li>
<li>GELU,</li>
<li>Swish, and</li>
<li>Step (for inference only).</li>
</ul>
<p>Please submit a pull request or an issue to add or request other optimizers, network types, or activation functions.</p>
<h2 id="getting-started">Getting Started</h2>
<h3 id="examples-and-demonstration-applications">Examples and demonstration applications</h3>
<p>The <a href="https://github.com/BerkeleyLab/fiats/blob/main/example">example</a> subdirectory contains demonstrations of several relatively simple use cases.
We recommend reviewing the examples to see how to handle basic tasks such as configuring a network training run or reading a neural network and using it to perform inference.</p>
<p>The <a href="https://github.com/BerkeleyLab/fiats/blob/main/demo">demo</a> subdirectory contains demonstration applications that depend on Fiats but build separately due to requiring additional prerequisites such as <a href="https://www.unidata.ucar.edu/software/netcdf/">NetCDF</a> and <a href="https://www.hdfgroup.org/solutions/hdf5/">HDF5</a>.
The demonstration applications
 - Train a cloud microphysics model surrogate for the Intermediate Complexity Atmospheric Research (<a href="https://github.com/BerkeleyLab/icar/tree/neural-net">ICAR</a>) package,
 - Perform inference using a pretrained model for aerosol dynamics in the Energy Exascale Earth System (<a href="https://e3sm.org">E3SM</a>) package, and
 - Calculate ICAR cloud microphysics tensor component statistics that provide useful insights for training-data reduction.</p>
<h3 id="building-and-testing">Building and Testing</h3>
<p>Because this repository supports programming language research, the code exercises new language features in novel ways.
We recommend using any compiler's latest release or even building open-source compilers from source.
The <a href="https://github.com/rouson/handy-dandy/blob/main/src">handy-dandy</a> repository contains scripts capturing steps for building the <a href="https://github.com/llvm/llvm-project">LLVM</a> compiler suite.
The remainder of this section contains commands for building Fiats with a recent Fortran compiler and the Fortran Package Manager ([<code>fpm</code>]).</p>
<h4 id="dependencies">Dependencies</h4>
<ul>
<li><a href="https://github.com/fortran-lang">Fortran Package Manager</a> (<code>fpm</code>)</li>
<li><a href="https://go.lbl.gov/julienne">Julienne</a> (automatically downloaded by <code>fpm</code>)</li>
<li><a href="https://go.lbl.gov/assert">Assert</a> (automatically downloaded by <code>fpm</code>)</li>
<li>A supported Fortran compiler:</li>
<li>LLVM (<code>flang-new</code>) version 19 or higher</li>
<li>NAG (<code>nagfor</code>) version 7.2 Build 7235 or higher</li>
<li>Intel (<code>ifx</code>) version 2025.2 or higher</li>
</ul>
<h4 id="supported-compilers">Supported Compilers</h4>
<h5 id="llvm-flang-new">LLVM (<code>flang-new</code>)</h5>
<p>Testing Fiats with LLVM <code>flang-new</code> version 20 or later with the following command should report that all tests pass:</p>
<div class="codehilite"><pre><span></span><code>fpm test --compiler flang-new --flag &quot;-O3&quot;
</code></pre></div>

<p>With LLVM 19, add <code>-mmlir -allow-assumed-rank</code> to the <code>--flag</code> argument.
With LLVM 21, it should be possible to automatically parallelize batch inferences on central processing units (CPUs).
For example, run <a href="./example/concurrent-inferences.f90">concurrent-inferences</a> in parallel as follows:</p>
<div class="codehilite"><pre><span></span><code>fpm run \
  --example concurrent-inferences \
  --compiler flang-new \
  --flag &quot;-O3 -fopenmp -fdo-concurrent-to-openmp=host&quot; \
  -- --network model.json
</code></pre></div>

<p>where <code>model.json</code> must be a neural network in the <a href="https://www.json.org/json-en.html">JSON</a> format used by Fiats and the companion <a href="https://go.lbl.gov/nexport">nexport</a> package.
<a href="https://dx.doi.org/10.25344/S4VG6T">Rouson et al. (2025)</a> demonstrated that this approach achieves performance on par with using OpenMP compiler directives.
Work is under way to automatically parallelize training on CPUs and to offload inference and training to GPUs.</p>
<h5 id="nag-nagfor">NAG (<code>nagfor</code>)</h5>
<p>With <code>nagfor</code> 7.2 Build 7235 or later, the following command builds Fiats and reports that all tests pass:</p>
<div class="codehilite"><pre><span></span><code>fpm test --compiler nagfor --flag -fpp
</code></pre></div>

<h4 id="partially-supported-compilers">Partially Supported Compilers</h4>
<p>Fiats release 0.14.0 and earlier support the use of the GNU and Intel Fortran compilers.
We are corresponding with these compilers' developers about addressing the issues preventing building newer Fiats releases.</p>
<h5 id="gnu-gfortran">GNU (<code>gfortran</code>)</h5>
<p>Compiler bugs related to parameterized derived types currently prevent <code>gfortran</code> from building Fiats versions 0.15.0 or later.
Test and build earlier versions of Fiats build with the following command:</p>
<div class="codehilite"><pre><span></span><code>fpm test --compiler gfortran --profile release
</code></pre></div>

<h5 id="intel-ifx">Intel (<code>ifx</code>)</h5>
<p>Testing with <code>ifx</code> versions higher than 2025.1 with the following command should report all tests passing except one:</p>
<div class="codehilite"><pre><span></span><code>fpm test --compiler ifx --flag -fpp --profile release
</code></pre></div>

<p>The failing test converts a neural network with varying-width hidden layers to and from JSON.
The reason for this failure is under investigation.
Please submit an issue if you would like to use <code>ifx</code> and require hidden layers of varying width.</p>
<h5 id="experimental-automatic-offloading-of-do-concurrent-to-gpus"><em>Experimental:</em> Automatic offloading of <code>do concurrent</code> to GPUs</h5>
<p>This capability is under development with the goal to facilitate automatic GPU offloading via the following command:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">fpm</span><span class="w"> </span><span class="nv">test</span><span class="w"> </span><span class="o">--</span><span class="nv">compiler</span><span class="w"> </span><span class="nv">ifx</span><span class="w"> </span><span class="o">--</span><span class="nv">profile</span><span class="w"> </span><span class="nv">release</span><span class="w"> </span><span class="o">--</span><span class="nv">flag</span><span class="w"> </span><span class="s2">&quot;-fopenmp-target-do-concurrent -qopenmp -fopenmp-targets=spir64 -O3&quot;</span>
</code></pre></div>

<h4 id="under-development">Under Development</h4>
<p>We are corresponding with the developers of the compiler(s) below about addressing the compiler issues preventing building Fiats.</p>
<h4 id="hpe-cray-compiler-environment-cce-crayftnsh">HPE Cray Compiler Environment (CCE) (<code>crayftn.sh</code>)</h4>
<p>Building with the CCE <code>ftn</code> compiler wrapper requires an additional trivial wrapper.
For example, create a file <code>crayftn.sh</code> with the following contents and place this file's location in your <code>PATH</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="ch">#!/bin/bash</span>

ftn<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</code></pre></div>

<p>Then execute</p>
<div class="codehilite"><pre><span></span><code>fpm test --compiler crayftn.sh
</code></pre></div>

<h3 id="configuring-a-training-run">Configuring a training run</h3>
<p>Fiats imports hyperparameters and network configurations to and from JSON files.
To see the expected file format, run the [print-training-configuration] example as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="c">% fpm run --example print-training-configuration --compiler gfortran</span>
</code></pre></div>

<p>which should produce output like the following:</p>
<div class="codehilite"><pre><span></span><code>Project is up to date
{
    &quot;hyperparameters&quot;: {
        &quot;mini-batches&quot; : 10,
        &quot;learning rate&quot; : 1.5,
        &quot;optimizer&quot; : &quot;adam&quot;
    }
,
    &quot;network configuration&quot;: {
        &quot;skip connections&quot; : false,
        &quot;nodes per layer&quot; : [2,72,2],
        &quot;activation function&quot; : &quot;sigmoid&quot;
    }
,
    &quot;tensor names&quot;: {
        &quot;inputs&quot;  : [&quot;pressure&quot;,&quot;temperature&quot;],
        &quot;outputs&quot; : [&quot;saturated mixing ratio&quot;]
    },  
     &quot;training data file names&quot;: {
         &quot;path&quot; : &quot;dates-20101001-2011076&quot;,
         &quot;inputs prefix&quot;  : &quot;training_input-image-&quot;,
         &quot;outputs prefix&quot; : &quot;training_output-image-&quot;,
         &quot;infixes&quot; : [&quot;000001&quot;, &quot;000002&quot;, &quot;000003&quot;, &quot;000004&quot;, &quot;000005&quot;, &quot;000006&quot;, &quot;000007&quot;, &quot;000008&quot;, &quot;000009&quot;, &quot;000010&quot;]
     }   
}
</code></pre></div>

<p>The Fiats JSON file format is fragile: splitting or combining lines breaks the file reader.
It should be ok, however, to add or removed white space or to reordered whole objects such as placing the "network configuration" object above the  "hyperparameters" object.
A future release will leverage the <a href="https://gitlab.com/everythingfunctional/rojff">rojff</a> JSON interface to allow for more flexible file formatting.</p>
<h3 id="training-a-neural-network">Training a neural network</h3>
<p>Running the following command will train a neural network to learn the saturated mixing ratio function that is one component of the ICAR SB04 cloud microphysics model (see the <a href="https://github.com/BerkeleyLab/fiats/blob/main/example/supporting-modules/saturated_mixing_ratio_m.f90">saturated_mixing_ratio_m</a> module for an implementation of the involved function):</p>
<div class="codehilite"><pre><span></span><code> fpm run --example learn-saturated-mixing-ratio --compiler gfortran --profile release -- --output-file sat-mix-rat.json
</code></pre></div>

<p>The following is representative output after 3000 epochs:</p>
<div class="codehilite"><pre><span></span><code> Initializing a new network
         Epoch | Cost Function| System_Clock | Nodes per Layer
         1000    0.79896E-04     4.8890      2,4,72,2,1
         2000    0.61259E-04     9.8345      2,4,72,2,1
         3000    0.45270E-04     14.864      2,4,72,2,1
</code></pre></div>

<p>The example program halts execution after reaching a cost-function threshold (which requires millions of epochs) or a maximum number of iterations or if the program detects a file named <code>stop</code> in the source-tree root directory.
Before halting, the program will print a table of expected and predicted saturated mixing ratio values across a range of input pressures and temperatures, wherein two the inputs have each been mapped to the unit interval [0,1].
The program also writes the neural network initial condition to <code>initial-network.json</code> and the final (trained) network to the file specified in the above command: <code>sat-mix-rat.json</code>.</p>
<h3 id="performing-inference">Performing inference</h3>
<p>Users with a PyTorch model may use <a href="https://go.lbl.gov/nexport">nexport</a> to export the model to JSON files that Fiats can read.
Examples of performing inference using a neural-network JSON file are in <a href="https://github.com/BerkeleyLab/fiats/blob/main/example/concurrent-inferences.f90">example/concurrent-inferences</a>.</p>
<h2 id="documentation">Documentation</h2>
<h3 id="html">HTML</h3>
<p>Please see our <a href="https://berkeleylab.github.io/fiats/">GitHub Pages site</a> for Hypertext Markup Languge (HTML) documentation generated by [<code>ford</code>] or generate documentation locally by installing <code>ford</code> and executing <code>ford ford.md</code>.</p>
<h3 id="uml">UML</h3>
<p>Please see the <a href="https://github.com/BerkeleyLab/fiats/blob/main/doc/uml"><code>doc/uml</code></a> subdirectory for Unified Modeling Language (UML) diagrams such as a comprehensive Fiats <a href="https://github.com/BerkeleyLab/fiats/blob/main/doc/uml/class-diagram.md">class diagram</a> with human-readable <a href="https://mermaid.js.org">Mermaid</a> source that renders graphically when opened by browsing to the document on GitHub.</p>
        </div>
          <div class="col-md-4">
            <div class="card card-body bg-light">
              <h2 class="card-title">Developer Info</h2>
              <h4 class="card-text">Berkeley Lab</h4>
              <p class="card-text"></p>
                <div class="text-center"><div class="btn-group" role="group">
                    
                    <a class="btn btn-lg btn-primary" href="https://github.com/berkeleylab"><i class="fa fa-github fa-lg"></i></a>
                    
                    
                    
                    
                    
                    
                </div></div>
            </div>
          </div>
      </div>
        <div class="row">
          <hr>
          <div class="col-xs-6 col-sm-3">
            <div>
              <h3>Source Files</h3>
              <ul><li><a href='sourcefile/activation_m.f90.html'>activation_m.f90</a></li><li><a href='sourcefile/activation_s.f90.html'>activation_s.F90</a></li><li><a href='sourcefile/concurrent-inferences.f90.html'>concurrent-inferences.f90</a></li><li><a href='sourcefile/double_precision_file_m.f90.html'>double_precision_file_m.f90</a></li><li><a href='sourcefile/double_precision_file_s.f90.html'>double_precision_file_s.f90</a></li><li><a href='sourcefile/double_precision_string_m.f90.html'>double_precision_string_m.f90</a></li><li><a href='sourcefile/double_precision_string_s.f90.html'>double_precision_string_s.f90</a></li><li><a href='sourcefile/fiats_m.f90.html'>fiats_m.f90</a></li><li><a href='sourcefile/hyperparameters_m.f90.html'>hyperparameters_m.f90</a></li><li><a href='sourcefile/hyperparameters_s.f90.html'>hyperparameters_s.F90</a></li></ul>
            </div>
            <div>
              <ul>
                <li><a href="./lists/files.html"><em>All source files&hellip;</em></a></li>
              </ul>
            </div>
          </div>
          <div class="col-xs-6 col-sm-3">
            <div>
              <h3>Modules</h3>
              <ul><li><a href='module/activation_m.html'>activation_m</a></li><li><a href='module/addition_m.html'>addition_m</a></li><li><a href='module/double_precision_file_m.html'>double_precision_file_m</a></li><li><a href='module/double_precision_string_m.html'>double_precision_string_m</a></li><li><a href='module/exponentiation_m.html'>exponentiation_m</a></li><li><a href='module/fiats_m.html'>fiats_m</a></li><li><a href='module/hyperparameters_m.html'>hyperparameters_m</a></li><li><a href='module/input_output_pair_m.html'>input_output_pair_m</a></li><li><a href='module/kind_parameters_m.html'>kind_parameters_m</a></li><li><a href='module/layer_m.html'>layer_m</a></li></ul>
            </div>
            <div>
              <ul>
                <li><a href="./lists/modules.html"><em>All modules&hellip;</em></a></li>
              </ul>
            </div>
          </div>
          <div class="col-xs-6 col-sm-3">
            <div>
              <h3>Procedures</h3>
              <ul><li><a href='interface/activation_name.html'>activation_name</a></li><li><a href='interface/activation_name~2.html'>activation_name</a></li><li><a href='interface/activation_t.html'>activation_t</a></li><li><a href='interface/assert_conformable.html'>assert_conformable</a></li><li><a href='interface/assert_consistency.html'>assert_consistency</a></li><li><a href='interface/default_real_activation.html'>default_real_activation</a></li><li><a href='interface/default_real_activation_name.html'>default_real_activation_name</a></li><li><a href='interface/default_real_allocate.html'>default_real_allocate</a></li><li><a href='interface/default_real_allocated.html'>default_real_allocated</a></li><li><a href='interface/default_real_approximately_equal.html'>default_real_approximately_equal</a></li></ul>
            </div>
            <div>
              <ul>
                <li><a href="./lists/procedures.html"><em>All procedures&hellip;</em></a></li>
              </ul>
            </div>
          </div>
          <div class="col-xs-6 col-sm-3">
            <div>
              <h3>Derived Types</h3>
              <ul><li><a href='type/activation_t.html'>activation_t</a></li><li><a href='type/double_precision_file_t.html'>double_precision_file_t</a></li><li><a href='type/double_precision_string_t.html'>double_precision_string_t</a></li><li><a href='type/hyperparameters_t.html'>hyperparameters_t</a></li><li><a href='type/input_output_pair_t.html'>input_output_pair_t</a></li><li><a href='type/layer_t.html'>layer_t</a></li><li><a href='type/metadata_t.html'>metadata_t</a></li><li><a href='type/mini_batch_t.html'>mini_batch_t</a></li><li><a href='type/network_configuration_t.html'>network_configuration_t</a></li><li><a href='type/neural_network_t.html'>neural_network_t</a></li></ul>
            </div>
            <div>
              <ul>
                <li><a href="./lists/types.html"><em>All derived types&hellip;</em></a></li>
              </ul>
            </div>
          </div>
        </div>
      <hr>
    </div> <!-- /container -->
    <footer>
      <div class="container">
        <div class="row justify-content-between">
          <div class="col">
            <p>
              Fiats
 was developed by Berkeley Lab<br>              &copy; 2025 
</p>
          </div>
          <div class="col">
            <p class="text-end">
              Documentation generated by
              <a href="https://github.com/Fortran-FOSS-Programmers/ford">FORD</a>
 on 2025-11-01 15:30 +0000             </p>
          </div>
        </div>
        <br>
      </div> <!-- /container -->
    </footer>
  </body>
</html>